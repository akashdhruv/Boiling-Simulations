#!/bin/bash

#SBATCH -A won_lab
#SBATCH -p standard
#SBATCH --nodes=2
#SBATCH --ntasks=6
#SBATCH --ntasks-per-node=3
#SBATCH -t 0-00:30
#SBATCH --job-name=pool_boiling

JobWorkDir="/dfs6/pub/fandi/p_test1/Boiling-Simulations/simulation/PoolBoiling/example2D"

cd /dfs6/pub/fandi/p_test1/Boiling-Simulations

# Bash file to load modules and set
# environment variables for compilers
# and external libraries

# MPI module. This should be available
# as standard module on a cluster. If not,
# build your own MPI and set
# PATH, LD_LIBRARY_PATH

# Flash-X requires that an environment variable
# MPI_HOME is available that points to the
# path to MPI installation
module load openmpi/4.1.2/gcc.8.4.0

# Export project home
export PROJECT_HOME=$(realpath .)

# Path to parallel HDF5 installtion with fortran support
export HDF5_HOME="$(realpath software/HDF5)/install"

# Store path to amrex as environment variable
export AMREX2D_HOME="$(realpath software/AMReX)/install/2D"
export AMREX3D_HOME="$(realpath software/AMReX)/install/3D"

# Path to Flash-X
export FLASH_HOME=$(realpath software/Flash-X)

# Path to Flash-X site/makefile
FlashSite=$(realpath sites/hello)

cd /dfs6/pub/fandi/p_test1/Boiling-Simulations/simulation/PoolBoiling/example2D

# cache the value of current directory
NodeDir=$(realpath .)

# Run pre-processing scripts located in the current directory
cd $JobWorkDir && python3 $NodeDir/poolBoilingHeater.py

cd /dfs6/pub/fandi/p_test1/Boiling-Simulations/simulation/PoolBoiling/example2D

# Run the actualy job using this target script
cd $JobWorkDir && mpirun $JobWorkDir/job.target -par_file job.input
